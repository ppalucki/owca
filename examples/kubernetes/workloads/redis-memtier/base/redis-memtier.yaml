apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: redis-memtier
spec:
  replicas: 0
  serviceName: redis-memtier
  podManagementPolicy: Parallel
  selector:
    matchLabels:
      app: redis-memtier
  template:
    metadata:
      labels:
        app: redis-memtier
    spec:
      terminationGracePeriodSeconds: 0
      nodeSelector:
        goal: service
      containers:
        - name: redis
          image: redis
          imagePullPolicy: Always
          securityContext:
            privileged: true
          envFrom:
            - configMapRef:
                name: redis
          env:
            - name: podname
              valueFrom: {fieldRef: {fieldPath: metadata.name}}
          command:
            - sh
            - -c
            - >
              echo 1024 >/proc/sys/net/core/somaxconn

              redis-server
              --save ''
              --loglevel
              notice
              --databases 1
              $redis_extra
        - name: memtier
          image: memtier_benchmark
          imagePullPolicy: Always
          securityContext:
            privileged: true
          envFrom:
            - configMapRef:
                name: memtier
          env:
            - name: podname
              valueFrom: {fieldRef: {fieldPath: metadata.name}}
          tty: true
          command:
            - bash
            - -c
            - >
              shopt -s extglob;
              identifier=${podname#memtier-};
              service=${identifier%-+([[:digit:]])};
              target='127.0.0.1';
              test=0;
              until [ "$test" = "PONG" ];
              do
                test=$(redis-cli PING);
                sleep 5;
              done

              # preparing data

              memtier_benchmark
              --run-count=1
              --requests=allkeys
              --server=$target
              --threads=1
              --clients=1
              --pipeline=20000
              --data-size=$datasize
              --hide-histogram
              --key-pattern=P:P
              --key-maximum=$keymaximum_gen
              --ratio 1:0
              &> /dev/null

              # Clear possible output from previous job on the same node

              echo "[RUN #0 0%,   0 secs]  0 threads:           0 ops,       0 (avg:       0) ops/sec, 0.00KB/sec (avg: 0.00KB/sec),  0.00 (avg:  0.00) msec latency";

              # loading

              stdbuf -e0 -o0
              memtier_benchmark
              --run-count=9999
              --requests=10000000000
              --server=$target
              --threads=$threads
              --hide-histogram
              --clients=$clients
              --pipeline=$pipeline
              --data-size=$datasize
              --key-pattern=G:G
              --key-maximum=$keymaximum_load
              --ratio $ratio
              2>&1 | stdbuf -e0 -o0 awk 'BEGIN { RS = "\r" } { print $0; fflush(); system("") }'

          volumeMounts:
            - name: var-log-wrapper
              mountPath: /var/log/wrapper

      volumes:
        - name: var-log-wrapper
          emptyDir: {}

  # required for workaround bug with kustomize https://github.com/kubernetes-sigs/kustomize/issues/504
  # when using commonLabels
  # fixed in 2.1.0 https://github.com/kubernetes-sigs/kustomize/commit/c470982ce5b96da82a757f088a842bb05d3bcdb4
  volumeClaimTemplates: []
